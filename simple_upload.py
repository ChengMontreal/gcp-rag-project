import os
import argparse
import sys
import glob  # æˆ‘ä»¬ç”¨ glob åº“æ¥æŸ¥æ‰¾æ–‡ä»¶
from langchain_community.document_loaders import PyPDFLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_google_vertexai import VertexAIEmbeddings
from langchain_google_vertexai.vectorstores import MatchingEngine

# --- é…ç½®åŒº ---
# (å’Œä¹‹å‰ä¸€æ ·ï¼Œä»ç¯å¢ƒå˜é‡è¯»å–)
try:
    PROJECT_ID = os.environ["PROJECT_ID"]
    REGION = os.environ["REGION"]
    BUCKET_NAME = os.environ["BUCKET_NAME"]
    ME_INDEX_ID = os.environ["ME_INDEX_ID_VALUE"]
    ME_INDEX_ENDPOINT_ID = os.environ["ME_INDEX_ENDPOINT_ID_VALUE"]
except KeyError as e:
    print(f"é”™è¯¯ï¼šç¯å¢ƒå˜é‡ {e} æœªè®¾ç½®ã€‚")
    print("è¯·ç¡®è®¤ä½ å·²ç»è¿è¡Œäº† 'source setup_env.sh' å¹¶ä¸”è„šæœ¬ä¸­çš„å˜é‡å·²æ­£ç¡®è®¾ç½®ã€‚")
    sys.exit(1)


# --- æ ¸å¿ƒåŠŸèƒ½ ---
def upload_single_pdf(file_path: str, vector_store: MatchingEngine):
    """åŠ è½½ã€åˆ†å‰²å•ä¸ªPDFï¼Œå¹¶å°†å‘é‡ä¸Šä¼ åˆ°å·²åˆå§‹åŒ–çš„ vector_store"""
    try:
        print(f"\nâ–¶ï¸ å¼€å§‹å¤„ç†æ–‡ä»¶: {os.path.basename(file_path)}")
        
        # 1. åŠ è½½ PDF
        loader = PyPDFLoader(file_path)
        documents = loader.load()
        
        # 2. åˆ†å‰²æ–‡æœ¬
        text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=100)
        docs = text_splitter.split_documents(documents)
        print(f"  - âœ‚ï¸ å·²åˆ†å‰²æˆ {len(docs)} ä¸ªæ–‡æœ¬å—ã€‚")

        # 3. æ·»åŠ æ–‡æ¡£ï¼ˆè¿™ä¼šè‡ªåŠ¨å¤„ç†å‘é‡ç”Ÿæˆå’Œä¸Šä¼ ï¼‰
        print(f"  - â³ æ­£åœ¨ä¸Šä¼ åˆ° Matching Engine...")
        vector_store.add_documents(docs)
        print(f"  - âœ… æˆåŠŸä¸Šä¼ : {os.path.basename(file_path)}")
        
    except Exception as e:
        print(f"  - âŒ å¤„ç†æ–‡ä»¶å¤±è´¥: {os.path.basename(file_path)}ã€‚é”™è¯¯: {e}")


# --- è„šæœ¬ä¸»å…¥å£ ---
if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="æ‰«æç›®å½•å¹¶ä¸Šä¼ æ‰€æœ‰PDFåˆ° Vertex AI Matching Engineã€‚")
    parser.add_argument(
        "directory",
        nargs="?",  # '?' è¡¨ç¤ºè¿™ä¸ªå‚æ•°æ˜¯å¯é€‰çš„
        default="./data",  # å¦‚æœä¸æä¾›å‚æ•°ï¼Œåˆ™é»˜è®¤ä¸º './data' ç›®å½•
        help="åŒ…å«PDFæ–‡ä»¶çš„ç›®å½•è·¯å¾„ã€‚é»˜è®¤ä¸º './data'ã€‚"
    )
    args = parser.parse_args()
    pdf_directory = args.directory

    if not os.path.isdir(pdf_directory):
        print(f"é”™è¯¯: ç›®å½• '{pdf_directory}' ä¸å­˜åœ¨ã€‚")
        sys.exit(1)

    # æŸ¥æ‰¾ç›®å½•ä¸­æ‰€æœ‰ .pdf å’Œ .PDF æ–‡ä»¶
    print(f"ğŸ” æ­£åœ¨æ‰«æç›®å½• '{pdf_directory}' ä¸­çš„PDFæ–‡ä»¶...")
    pdf_files = glob.glob(os.path.join(pdf_directory, "*.pdf"))
    pdf_files.extend(glob.glob(os.path.join(pdf_directory, "*.PDF")))

    if not pdf_files:
        print("ğŸ¤· åœ¨è¯¥ç›®å½•ä¸­æ²¡æœ‰æ‰¾åˆ°PDFæ–‡ä»¶ã€‚")
        sys.exit(0)

    print(f"âœ¨ æ‰¾åˆ°äº† {len(pdf_files)} ä¸ªPDFæ–‡ä»¶ï¼Œå‡†å¤‡å¼€å§‹å¤„ç†ã€‚")

    # ---- åˆå§‹åŒ–ä¸€æ¬¡ï¼Œä¾›æ‰€æœ‰æ–‡ä»¶ä½¿ç”¨ ----
    print("â˜ï¸ æ­£åœ¨åˆå§‹åŒ–å¹¶è¿æ¥åˆ° Vertex AI Matching Engine... (åªéœ€ä¸€æ¬¡)")
    embeddings_service = VertexAIEmbeddings(model_name="textembedding-gecko@003", project=PROJECT_ID)
    vector_store_instance = MatchingEngine.from_components(
        project_id=PROJECT_ID,
        region=REGION,
        gcs_bucket_name=BUCKET_NAME.replace("gs://", "").strip("/"),
        embedding=embeddings_service,
        index_id=ME_INDEX_ID,
        endpoint_id=ME_INDEX_ENDPOINT_ID,
    )
    print("ğŸ”— è¿æ¥æˆåŠŸï¼")
    # ------------------------------------

    # å¾ªç¯å¤„ç†æ‰¾åˆ°çš„æ¯ä¸ªæ–‡ä»¶
    for pdf_path in pdf_files:
        upload_single_pdf(pdf_path, vector_store_instance)

    print("\nğŸ‰ å…¨éƒ¨ä»»åŠ¡å®Œæˆï¼")
    